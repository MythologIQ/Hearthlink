INFO:offline_llm_manager:Loaded 5 models for offline redundancy
INFO:offline_llm_manager:Started offline LLM monitoring systems
INFO:__main__:LLM connected via http://localhost:11434 - 3 models available
Starting Hearthlink Local LLM API...
Endpoints available:
  GET  /api/health - Health check
  GET  /api/status - Detailed service status
  GET  /api/models - Available models and profiles
  POST /api/models/pull - Pull/download new model
  POST /api/chat - Chat with Local LLM
  GET  /api/profiles - Get dual LLM profiles
  PUT  /api/profiles - Update dual LLM profiles
  POST /api/test - Test connection and functionality
  GET  /api/metrics - Service metrics
  GET  /api/connection-pool - Connection pool status and stability metrics
  GET  /api/circuit-breakers/status - Get circuit breaker status
  POST /api/circuit-breakers/<service_name>/reset - Reset specific circuit breaker
  POST /api/circuit-breakers/reset-all - Reset all circuit breakers

Starting Local LLM API server on port 8001...
 * Serving Flask app 'local_llm_api'
 * Debug mode: on
WARNING:offline_llm_manager:Only 0 models available (minimum: 2)
INFO:offline_llm_manager:Ensuring offline model availability...
INFO:offline_llm_manager:Downloading emergency model: llama3.2-1b
INFO:offline_llm_manager:Starting download of model: llama3.2-1b
ERROR:offline_llm_manager:Error downloading model llama3.2-1b: [Errno 2] No such file or directory: 'ollama'
INFO:offline_llm_manager:Downloading emergency model: phi3-mini
INFO:offline_llm_manager:Starting download of model: phi3-mini
ERROR:offline_llm_manager:Error downloading model phi3-mini: [Errno 2] No such file or directory: 'ollama'
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8001
 * Running on http://192.168.0.29:8001
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with stat
INFO:offline_llm_manager:Loaded 5 models for offline redundancy
INFO:offline_llm_manager:Started offline LLM monitoring systems
INFO:__main__:LLM connected via http://localhost:11434 - 3 models available
WARNING:offline_llm_manager:Only 0 models available (minimum: 2)
INFO:offline_llm_manager:Ensuring offline model availability...
INFO:offline_llm_manager:Downloading emergency model: llama3.2-1b
INFO:offline_llm_manager:Starting download of model: llama3.2-1b
ERROR:offline_llm_manager:Error downloading model llama3.2-1b: [Errno 2] No such file or directory: 'ollama'
INFO:offline_llm_manager:Downloading emergency model: phi3-mini
INFO:offline_llm_manager:Starting download of model: phi3-mini
ERROR:offline_llm_manager:Error downloading model phi3-mini: [Errno 2] No such file or directory: 'ollama'
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 400-997-232
INFO:circuit_breaker:Circuit breaker 'ollama_service' initialized with config: CircuitBreakerConfig(failure_threshold=3, recovery_timeout=30, success_threshold=2, timeout=45, monitoring_window=300)
INFO:circuit_breaker:Created new circuit breaker for service: ollama_service
ERROR:circuit_breaker:Circuit breaker 'ollama_service' recorded failure: 'max_tokens'
ERROR:__main__:Chat endpoint error: 'max_tokens'
INFO:werkzeug:127.0.0.1 - - [20/Jul/2025 21:30:07] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
WARNING:offline_llm_manager:Only 0 models available (minimum: 2)
INFO:offline_llm_manager:Ensuring offline model availability...
INFO:offline_llm_manager:Downloading emergency model: llama3.2-1b
INFO:offline_llm_manager:Starting download of model: llama3.2-1b
ERROR:offline_llm_manager:Error downloading model llama3.2-1b: [Errno 2] No such file or directory: 'ollama'
INFO:offline_llm_manager:Downloading emergency model: phi3-mini
INFO:offline_llm_manager:Starting download of model: phi3-mini
ERROR:offline_llm_manager:Error downloading model phi3-mini: [Errno 2] No such file or directory: 'ollama'
WARNING:offline_llm_manager:Only 0 models available (minimum: 2)
INFO:offline_llm_manager:Ensuring offline model availability...
INFO:offline_llm_manager:Downloading emergency model: llama3.2-1b
INFO:offline_llm_manager:Starting download of model: llama3.2-1b
ERROR:offline_llm_manager:Error downloading model llama3.2-1b: [Errno 2] No such file or directory: 'ollama'
INFO:offline_llm_manager:Downloading emergency model: phi3-mini
INFO:offline_llm_manager:Starting download of model: phi3-mini
ERROR:offline_llm_manager:Error downloading model phi3-mini: [Errno 2] No such file or directory: 'ollama'
INFO:werkzeug: * Detected change in '/mnt/g/MythologIQ/Hearthlink/src/api/local_llm_api.py', reloading
Starting Hearthlink Local LLM API...
Endpoints available:
  GET  /api/health - Health check
  GET  /api/status - Detailed service status
  GET  /api/models - Available models and profiles
  POST /api/models/pull - Pull/download new model
  POST /api/chat - Chat with Local LLM
  GET  /api/profiles - Get dual LLM profiles
  PUT  /api/profiles - Update dual LLM profiles
  POST /api/test - Test connection and functionality
  GET  /api/metrics - Service metrics
  GET  /api/connection-pool - Connection pool status and stability metrics
  GET  /api/circuit-breakers/status - Get circuit breaker status
  POST /api/circuit-breakers/<service_name>/reset - Reset specific circuit breaker
  POST /api/circuit-breakers/reset-all - Reset all circuit breakers

Starting Local LLM API server on port 8001...
INFO:werkzeug: * Restarting with stat
INFO:offline_llm_manager:Loaded 5 models for offline redundancy
INFO:offline_llm_manager:Started offline LLM monitoring systems
INFO:__main__:LLM connected via http://localhost:11434 - 3 models available
WARNING:offline_llm_manager:Only 0 models available (minimum: 2)
INFO:offline_llm_manager:Ensuring offline model availability...
INFO:offline_llm_manager:Downloading emergency model: llama3.2-1b
INFO:offline_llm_manager:Starting download of model: llama3.2-1b
ERROR:offline_llm_manager:Error downloading model llama3.2-1b: [Errno 2] No such file or directory: 'ollama'
INFO:offline_llm_manager:Downloading emergency model: phi3-mini
INFO:offline_llm_manager:Starting download of model: phi3-mini
ERROR:offline_llm_manager:Error downloading model phi3-mini: [Errno 2] No such file or directory: 'ollama'
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 400-997-232
WARNING:offline_llm_manager:Only 0 models available (minimum: 2)
INFO:offline_llm_manager:Ensuring offline model availability...
INFO:offline_llm_manager:Downloading emergency model: llama3.2-1b
INFO:offline_llm_manager:Starting download of model: llama3.2-1b
ERROR:offline_llm_manager:Error downloading model llama3.2-1b: [Errno 2] No such file or directory: 'ollama'
INFO:offline_llm_manager:Downloading emergency model: phi3-mini
INFO:offline_llm_manager:Starting download of model: phi3-mini
ERROR:offline_llm_manager:Error downloading model phi3-mini: [Errno 2] No such file or directory: 'ollama'
