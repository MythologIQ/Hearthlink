# ğŸ”§ LLM Backend Configuration Guide

Hearthlink supports multiple LLM backends for maximum versatility. Choose the one that fits your setup.

---

## ğŸ¯ Quick Setup by User Type

### Current User (Claude Code)
```bash
# No setup needed! Already configured for Claude Code CLI
cd /mnt/g/MythologIQ/Hearthlink/src
python run_services.py
```

### API Users (Future)
```bash
# Claude API
export CLAUDE_API_KEY="sk-ant-api03-..."

# ChatGPT API  
export OPENAI_API_KEY="sk-..."

# Start services
python run_services.py
```

### Local LLM Users
```bash
# Start Ollama
ollama serve
ollama pull llama2

# Configure Hearthlink
# Edit config/llm_backends.json - set local_llm.enabled = true
python run_services.py
```

---

## ğŸ—ï¸ Backend Options

### 1. **Claude Code CLI** (Current Setup)
- âœ… **No API key needed**
- âœ… **Already authenticated**  
- âœ… **Default backend**
- ğŸ“ Uses your existing Claude Code session

### 2. **Claude API** (For API Users)
- ğŸ”‘ Requires Anthropic API key
- ğŸ’° Paid per token usage
- ğŸš€ Direct API access
- ğŸ“Š Full token tracking

### 3. **ChatGPT API** (For OpenAI Users)
- ğŸ”‘ Requires OpenAI API key
- ğŸ’° Paid per token usage  
- ğŸ¤– GPT-4 models available
- ğŸ”„ Alternative to Claude

### 4. **Local LLM** (Privacy First)
- ğŸ  Runs entirely local
- ğŸ†“ No API costs
- ğŸ”’ Complete privacy
- âš¡ Works with Ollama/LMStudio

### 5. **Hearthlink API** (Reverse Connection)
- ğŸ”— Connect to external Hearthlink
- ğŸŒ Distributed processing
- ğŸ”„ Load balancing
- ğŸ¢ Enterprise setups

---

## ğŸ“‹ Configuration Files

### Main Config: `config/llm_backends.json`
```json
{
  "default_backend": "claude-code",
  "backends": {
    "claude_code": {
      "enabled": true,
      "description": "Claude Code CLI integration"
    }
  }
}
```

### Environment Variables
```bash
# Optional - only if using API backends
export CLAUDE_API_KEY="your-claude-key"
export OPENAI_API_KEY="your-openai-key"  
export HEARTHLINK_API_ENDPOINT="https://api.hearthlink.com"
```

---

## ğŸ”„ Switching Backends

### Runtime Switching (Future Feature)
```typescript
// In React components
const { switchBackend, getAvailableBackends } = useClaudeConnector();

// Switch to different backend
switchBackend('chatgpt-api');

// Check available backends
const backends = getAvailableBackends(); // ['claude-code', 'local_llm']
```

### Configuration Switching
```bash
# Edit config/llm_backends.json
{
  "default_backend": "local_llm",  # Changed from "claude-code"
  "backends": {
    "local_llm": {
      "enabled": true             # Enable desired backend
    }
  }
}
```

---

## ğŸš€ Backend Implementation Status

| Backend | Status | Current User | Future Users |
|---------|--------|--------------|--------------|
| **Claude Code** | âœ… Ready | âœ… Active | âœ… Supported |
| **Claude API** | âœ… Ready | âŒ N/A | âœ… Supported |
| **ChatGPT API** | âœ… Ready | âŒ N/A | âœ… Supported |
| **Local LLM** | âœ… Ready | âš ï¸ Optional | âœ… Supported |
| **Hearthlink API** | âœ… Ready | âŒ N/A | âœ… Supported |

---

## ğŸ› ï¸ Adding New Backends

### 1. Create Backend Class
```typescript
class MyLLMBackend {
  async generate(request: LLMRequest): Promise<LLMResponse> {
    // Your LLM integration here
  }
  
  async healthCheck() {
    // Health check implementation
  }
}
```

### 2. Register in BackendManager
```typescript
// In LLMBackendManager.ts
this.backends.set('my-llm', new MyLLMBackend(config));
```

### 3. Add Configuration
```json
{
  "backends": {
    "my_llm": {
      "enabled": false,
      "config": { "endpoint": "..." }
    }
  }
}
```

---

## ğŸ” Troubleshooting

### Backend Not Available
```bash
# Check backend status
curl http://localhost:8080/api/backends/status

# Check logs
tail -f logs/synapse.log
```

### Claude Code Issues
```bash
# Check Claude Code auth
claude auth status

# Re-authenticate if needed
claude auth login
```

### Local LLM Issues
```bash
# Check Ollama
ollama list
ollama serve

# Check LMStudio
curl http://localhost:1234/v1/models
```

---

## ğŸ¯ Recommendations

### For Current User
- âœ… **Keep Claude Code backend** - already working
- âš ï¸ **Optional**: Set up local LLM as backup
- ğŸ“š **Future**: Easy to add API backends later

### For Future Users  
- ğŸ”‘ **API backends** for production use
- ğŸ  **Local LLM** for privacy/cost concerns
- ğŸ”„ **Multiple backends** for redundancy

### For Enterprise
- ğŸŒ **Hearthlink API** for distributed processing
- ğŸ”’ **Local LLM** for sensitive data
- ğŸ“Š **Full token tracking** across all backends

The versatile architecture ensures Hearthlink works for **every user type and use case**.